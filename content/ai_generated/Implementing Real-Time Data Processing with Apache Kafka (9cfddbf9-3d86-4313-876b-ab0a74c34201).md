# Implementing Real-Time Data Processing with Apache Kafka

Apache Kafka is a distributed event streaming platform that can handle real-time
data processing and high-throughput messaging.

It allows you to publish and subscribe to streams of records, process streams of
records in real-time, and store records in a fault-tolerant manner. This guide
covers the basics of setting up and using Apache Kafka for real-time data
processing.

## Key Concepts of Apache Kafka

1. **Topics**: Categories or feeds to which records are sent. Producers write
   data to topics, and consumers read from them.
2. **Partitions**: A topic is split into partitions to allow parallel processing
   and improve scalability.
3. **Brokers**: Kafka servers that store data and serve client requests. A Kafka
   cluster is made up of multiple brokers.
4. **Producers**: Applications that publish records to Kafka topics.
5. **Consumers**: Applications that read records from Kafka topics.
6. **ZooKeeper**: A service used by Kafka to manage and coordinate distributed
   brokers.

## Setting Up Apache Kafka

### 1. Download and Install Kafka

Download the latest Kafka release from the
[official website](https://kafka.apache.org/downloads):

```bash
wget https://downloads.apache.org/kafka/{version}/kafka_{version}.tgz
tar -xzf kafka_{version}.tgz
cd kafka_{version}
```

### 2. Start ZooKeeper and Kafka Brokers

Kafka uses ZooKeeper for managing cluster metadata. Start ZooKeeper and Kafka
brokers in separate terminals:

- **Start ZooKeeper**

```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
```

- **Start Kafka Broker**

```bash
bin/kafka-server-start.sh config/server.properties
```

### 3. Create a Kafka Topic

Create a topic named my-topic:

```bash
bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
```

Verify that the topic was created:

```bash
bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

### 4. Produce and Consume Messages

- **Produce Messages:**

Send messages to my-topic using the Kafka console producer:

```bash
bin/kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092
```

Type messages and press Enter to send them.

- **Consume Messages:**

Read messages from my-topic using the Kafka console consumer:

```bash
bin/kafka-console-consumer.sh --topic my-topic --bootstrap-server localhost:9092 --from-beginning
```

### 5. Implement a Kafka Producer and Consumer in Code

- **Producer Example (Node.js):**

Install the kafka-node package:

```bash
npm install kafka-node
```

Create a producer.js file:

```js
const kafka = require('kafka-node')
const client = new kafka.KafkaClient({ kafkaHost: 'localhost:9092' })
const producer = new kafka.Producer(client)

producer.on('ready', () => {
  producer.send(
    [{ topic: 'my-topic', messages: 'Hello Kafka!' }],
    (err, data) => {
      if (err) console.error(err)
      else console.log('Message sent:', data)
    },
  )
})

producer.on('error', (err) => {
  console.error('Producer error:', err)
})
```

- **Consumer Example (Node.js):**

Create a consumer.js file:

```js
const kafka = require('kafka-node')
const client = new kafka.KafkaClient({ kafkaHost: 'localhost:9092' })
const consumer = new kafka.Consumer(client, [{ topic: 'my-topic' }], {
  autoCommit: true,
})

consumer.on('message', (message) => {
  console.log('Received message:', message.value)
})

consumer.on('error', (err) => {
  console.error('Consumer error:', err)
})
```

### 6. Monitor and Scale Kafka

- **Monitoring**: Use tools like Kafka Manager, Confluent Control Center, or
  Prometheus to monitor Kafka brokers and topics.
- **Scaling**: Add more partitions to topics and increase the number of brokers
  to handle higher throughput.

## Conclusion

Apache Kafka provides a robust and scalable solution for real-time data
processing. By setting up Kafka topics, producers, and consumers, you can
efficiently handle large streams of data.

Monitoring and scaling Kafka ensures that your data processing pipeline remains
performant and reliable as your data needs grow.
